# Frontend specs

As frontend has grown in the complexity and becomes an actual user-facing feature, it is worth separating it into the UI.

## Frontend and debugging infrastructure

I will need some frontend. I suggest designing a custom UI. This is relatively easy now because we can handle off to Google Stitch (Google's AI website designer; however it only designs websites, it doesn't integrate them). Plus it's convenient to use in backend. The website will be deployed on Vercel for simplicity (or a railway bucket maybe? it doesn't really matter.)
A detailed specification of what needs to be in frontend to create a good UI will be required.

This means that we will have a user-facing API.

<!-- I used streamlit in the past and it works but is limiting due to inability to stream data, as far as I've seen it.-->

### Debugging requirements

As the agents are long-running (and cost money!) it is desirable to be able to:

1. Submit requests one-by-one (as opposed to working in batch over a dataset of prompts)
2. View their reasoning prompts
3. Interrupt them before they finish.
<!-- 4. (dev only) using an environmental variable on all all nodes(dev_mode=True), fetch logs from either controller or worker (and maybe view in UI) (this may be easier as we use `structlog`) -->
1. Steer agents reasoning in a "chat" mode, meaning, correct their reasoning in case they made an incorrect assumption.

#### Benchmark generation workflow

I need a place to enable "create benchmark" functionality. So go from a high-level prompt to a benchmark plan, through the confirm and implementation.

Here's what I suggest: add a "+ Create new" primary button instead of "history" icon next to "benchmark runs". Upon this, the data from the the current will clear (from the UI, not from the DB), and we'll go into "benchmark creation" mode. Here the "traces" section will have a textbox on top of it, prompting to create a benchmark.

Afterwards (this should be the standard agent flow), the plan.md wil be created and placed into the folder directory. the plan file will have a "start implementation" on the top of it; with it the benchmark generation will start.
The UI will be automatically updated with new models as the build123d generation will progress.

#### Solution workflow

Engineers must be able to prompt solutions to benchmarks (this is a core workflow). They will select a benchmark that will be solved and will generate solutions to the benchmark.

#### Interrupting the worker and progress bars

If we want to stop the generation in the controller, it will also halt the job(s) in the workers.

Notably `deepagents` has a [support for this](https://docs.langchain.com/oss/python/deepagents/human-in-the-loop.md) - for reviewing and interruption.

#### Viewing code, plans, reviews, simulation

For both benchmark generator and engineer we want to view:

1. Markdown plans
2. Generated code (python, not MJCF) (as artifacts)
3. Final/starting renders; latest renders. (as artifacts)
4. Reasoning traces; meaning LLM reasoning steps and how it arrives to its actions.
5. 3d view of the generated model. Notably, it is desirable to

(additionally), history of the implementation.

##### Component layout

Benchmark generator and engineer viewer have a very similar structure (I wouldn't make them one piece of code yet, just similar/shared components)...

1. Sidebar panel on the left,
2. We separate the entire UI to 3 columns about 3:3:6 split - 3 is the current sidebar column, 3 for reasoning traces (traces streamed realtime), and 6 for the rightmost column (adjustable, of course.)
   - Note! This is by default. We allow to resize the dashboard pieces.

The rightmost column is split vertically into:

1. a 3d view on top (like it currently is)
2. an artifact view. Artifacts are plans, reviews etc. it's like a mini-tree + cards on top view - the file tree is collapsible and cards on top can be used to switch files.

##### CAD viewer

Turns out viewing CAD is not as easy. In addition, special-purpose CAD viewing functionality is desired - for being able to "click" on a face and prompt something to a model - we'll need it in the future.

Ideally, this is solved via WASM in the browser. But I can't give two craps about debugging WASM yet, e.g. a "Yet Another CAD viewer" which runs in WASM.

So, use a "Yet Another CAD Viewer" server-side rendering for now. integrate it into the worker, ideally.
The file format is GLB (not STL) because of lesser volume.

[!Note] Exception - the frontend will query the worker to get the GLB files. (we specified elsewhere that it will communicate only to controller first.) GET-method only though.
<!-- Note: in the future it should be WASM+b123d viewer in the browser though. Or at least a CAD viewer. -->

<!-- *Current hack*: the frontend would simply send the  -->

### Frontend architecture

Vite, React. Autogenerated types on git hooks from Controller. Super-modern, powerful look

### A set of nice UI features

I want a set of nice UI features in the app. they'll be also useful if we'll take the app to prod:

#### Action buttons

For each "action" there should be a nice render, kind of like a "edited: [icon] [file name]", and "[viewed: dir], similar to coding agents (because it is one)

### (important feature) Chat UI

The "reasoning trace" UI should be not a single message, but something iterative. An engineer should be able to say "I don't like X, it's unstable" and the planner will replan accordingly.

##### Collecting feedback from users

Users can submit thumbs up/down on model outputs, just as they would in most "chat" UIs.

The feedback would be when the model ends its output, and not at each message.

##### All tools are in chat

the "approve plan(and thus implement)" buttons should be also in the reasoning trace (or, I guess it should be named "chat window" now).. and all the "action" buttons should be in reasoning trace now. This text in particular that currently sits elsewhere
"""
Execution Plan
Review and approve the benchmark implementation steps.
"""

##### Code/text linting

The user will review code or markdown to the user that we generate. Lint them properly, and with colors. e.g. for markdown, color the headings, for python color functions, etc.

##### Proper icons

All files in code should have proper, colorful icons. Python icon for python files, yaml for aml files, etc.

When files are edited in the chat UI, the files have icons too.

### Config

Config - benchmark generation config, linting config, manufacturability and pricing config, and prompts will be stored in YAML files.
