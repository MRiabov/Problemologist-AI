# Feature Specification: Agentic CAD Dashboard & 3D Debugger

**Feature**: 007-agentic-cad-dashboard
**Status**: Draft
**Mission**: software-dev

## 1. Overview

This feature provides a **Streamlit-based visual dashboard** ("Human-in-the-Loop" UI) for the Problemologist-AI environment. It serves as the primary window into the agent's cognition and output, solving the critical problem of debugging 3D design tasks where text logs are insufficient.

The dashboard supports three modes: **Real-Time Monitoring** (streaming live agent thoughts and renders), **Historical Review** (browsing past episodes from the SQLite database), and **Benchmark Generator** (an interactive pipeline for co-creating physics puzzles). It mimics the layout of modern AI coding assistants (like Cursor) but adds a specialized **3D Viewer Pane** to interactively inspect the CAD models (STLs/Meshes) generated by the agent.

## 2. Goals & Success Criteria

### 2.1. Primary Goals

1.  **Visualize Cognition**: Display the agent's step-by-step reasoning (Chain-of-Thought), tool usage, and generated Python code in a readable, syntax-highlighted format.
2.  **Visualize Geometry**: Integrate an interactive 3D viewer (using `stpyvista` or `streamlit-model-viewer`) to render the agent's output meshes (STL/GLB) directly in the browser.
3.  **Unified Debugging**: Correlate code changes with visual outcomes side-by-side (e.g., "Step 5: Adjusted fillet radius" -> Show Updated Model).
4.  **Dual-Mode Access**: Enable users to watch live runs for immediate feedback and replay historical runs for deep analysis.
5.  **Human-in-the-Loop Co-Creation**: Provide a structured, multi-stage UI for the Benchmark Scenario Generator.

### 2.2. Success Criteria

*   **3D Rendering**: The dashboard can load and render an STL file generated by the agent within 2 seconds.
*   **Data Fidelity**: All steps from an episode (User Prompt -> Reasoning -> Code -> Output) are displayed correctly.
*   **Latency**: In "Real-Time" mode, the dashboard updates within 1 second of a new database entry being committed.
*   **Usability**: A user can navigate between different steps of a historical episode using a slider or sidebar.

## 3. User Stories

*   **As a Developer**, I want to see the exact Python code the agent wrote alongside the resulting 3D model so I can verify if the geometry matches the code intent.
*   **As a Developer**, I want to browse past failed episodes to understand *why* the agent failed (e.g., did it misunderstand the prompt or write invalid geometry code?).
*   **As a User**, I want to watch the agent "work" in real-time, seeing the model evolve step-by-step.
*   **As a User**, I want to rotate and zoom the 3D model to inspect specific details (like fillets or internal features) that aren't visible in static screenshots.
*   **As a Dataset Curator**, I want to interactively approve or edit benchmark plans and CAD code before they are finalized, ensuring high-quality ground-truth scenarios.

## 4. Functional Requirements

### 4.1. Dashboard Layout

*   **Sidebar Navigation**:
    *   **Mode Selection**: Users must be able to switch between "Viewer" (Live/History) and "Benchmark Generator" modes.
    *   **Episode Filtering**: Users must be able to search and select past episodes based on timestamp, prompt text, or success status.
    *   **Step Navigation**: Users must be able to sequentially step through an episode's timeline (e.g., using a slider or next/prev buttons) to view the state at any point.
*   **Main Workspace (Split View)**:
    *   **Cognition Pane (Left)**:
        *   **Chat Interface**: Display the conversation history, including User Prompts and Agent Reasoning (Chain-of-Thought).
        *   **Code Viewer**: Display the generated design script (e.g., Python) with syntax highlighting.
        *   **Execution Log**: Display system outputs (stdout/stderr) to reveal errors or warnings.
    *   **Artifact Pane (Right)**:
        *   **3D Viewport**: An interactive canvas displaying the geometric model corresponding to the current step.
        *   **Metrics Panel**: Display key performance indicators (e.g., Cost, Mass, Validation Status) for the current design iteration.

### 4.4. Benchmark Generator Interface

*   **Stage 1: Input**: A text area for the user to provide initial benchmark requirements.
*   **Stage 2: Plan Approval**: Display the generated Plan in an editable text area with Approve/Reject controls.
*   **Stage 3: CAD/Visual Review**: 
    *   Display the generated `build123d` code (editable).
    *   Display multiple rendering angles of the generated scenario.
    *   Provide a "Re-validate" action to run the stability check after manual code edits.
*   **Stage 4: Finalization**: Show the final MJCF XML and allow downloading of the benchmark artifacts.

### 4.2. 3D Visualization Capabilities

*   **Format Support**: The system must support loading and rendering standard 3D mesh formats (e.g., STL) generated by the agent.
*   **User Interaction**: The 3D viewport must support standard camera controls: Orbit (rotate), Zoom, and Pan.
*   **Visual Fidelity**: The renderer must be capable of displaying the geometry clearly enough to inspect features like fillets and holes.

### 4.3. Data Integration

*   **Data Source**: The dashboard must visualize data persisted in the system's primary storage (the "Thought-Process" database).
*   **Real-Time Updates**: In "Live" mode, the dashboard must automatically refresh to show the latest agent actions without requiring manual page reloads.
*   **Data Mapping**: The UI must correctly map and display all relevant step data: Input Prompt, Reasoning/Code, System Output, and resulting Artifacts (Files/Images).

## 5. Technical Design

### 5.1. Tech Stack

*   **Framework**: `streamlit` (Python) for rapid UI development.
*   **3D Rendering Library**: `stpyvista` (Streamlit PyVista wrapper) or `streamlit-components` with `three.js`. Backend processing via `trimesh`.
*   **Database Connector**: `sqlalchemy` (read-only connection to `history.db`).
*   **Syntax Highlighting**: Built-in `st.code` component.

### 5.2. Architecture

*   **Entry Point**: `dashboard.py`.
*   **Data Layer (`src/dashboard/data.py`)**: Responsible for querying `history.db`, polling for updates, and resolving relative file paths to absolute system paths.
*   **Component Layer (`src/dashboard/components.py`)**: Reusable UI widgets for the Chat Log, Code Editor view, and 3D Viewport wrapper.
*   **File Access**: The dashboard application requires read-access to the local `artifacts/` directory.

### 5.3. Implementation Details

*   **Polling Strategy**: Use `st.empty()` container with a `time.sleep()` loop for the live monitoring view.
*   **Schema Integration**: The dashboard will read from `Episodes`, `Steps`, and `Artifacts` tables defined in `src.persistence`.

## 6. Assumptions & Constraints

*   **Local Execution**: The dashboard runs locally and accesses the filesystem directly (no remote S3 fetching).
*   **Performance**: Large meshes (>10MB) might be slow to load in the browser; we assume MVP meshes are reasonably simple (<100k triangles).
*   **Database Locking**: SQLite single-writer rule applies; the dashboard must use `read_only` connections to avoid blocking the running agent.
